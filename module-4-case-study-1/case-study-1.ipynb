{"cells":[{"cell_type":"markdown","metadata":{"id":"40bZuxsQnQKv"},"source":["# <font color='black'>Data Sciente Academy</font>\n","## <font color='black'>Generative AI and LLMs for Natural Language Processing</font>\n","## <font color='black'>Study Case 1 - LLM for text generation - GPT-2-Large written in Python</font>"]},{"cell_type":"markdown","metadata":{"id":"so0u9Y_cZPpn"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"wJTMqDognQKy"},"source":["## Instalando e Carregando Pacotes"]},{"cell_type":"markdown","source":["Watermark allows to create a \"watermark\" with the current date and time, the version of Python, and the version of the Jupyter notebook. It is useful for keeping track of the version of the notebook and the environment in which it was run. This is useful for reproducibility and for sharing the notebook with others.\n","*   -q is for quiet mode, which suppresses output messages.\n","*   -U is for upgrade, which upgrades the package to the latest version."],"metadata":{"id":"XY4S1hRoUh1T"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"r57wAMPLnQKy","executionInfo":{"status":"ok","timestamp":1743561362211,"user_tz":180,"elapsed":6698,"user":{"displayName":"Lucas Lira","userId":"06092401788430501434"}}},"outputs":[],"source":["%pip install -q -U watermark"]},{"cell_type":"markdown","source":["Tensorflow is a library for machine learning and deep learning. It contains functions and classes for building and training neural networks including, convolutional neural networks (CNNs), recurrent neural networks (RNNs) and\n","transformer architecture."],"metadata":{"id":"k9DQLCfvU78K"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"cnCFqnQDnQKz","executionInfo":{"status":"ok","timestamp":1743561375699,"user_tz":180,"elapsed":5972,"user":{"displayName":"Lucas Lira","userId":"06092401788430501434"}}},"outputs":[],"source":["%pip install -q tensorflow==2.19.0"]},{"cell_type":"markdown","source":["A biblioteca Transformers do Python, criada pela Hugging Face, fornece ferramentas para uso simplificado de modelos de inteligência artificial baseados em transformers. É usada principalmente em NLP para tarefas como geração de texto, classificação, tradução e análise de sentimentos. Permite fácil integração de modelos pré-treinados, acelerando o desenvolvimento de soluções avançadas."],"metadata":{"id":"xFAGeNteVUYr"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"xMImzYVOnQKz","executionInfo":{"status":"ok","timestamp":1743561389833,"user_tz":180,"elapsed":6149,"user":{"displayName":"Lucas Lira","userId":"06092401788430501434"}}},"outputs":[],"source":["%pip install -q -U transformers==4.50.3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LKztIACiZPpu","outputId":"3ac2761d-4bc5-407b-f40d-783b8735657f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["#%pip install -q -U torch"]},{"cell_type":"markdown","metadata":{"id":"MonITO7eqKTF"},"source":["https://huggingface.co/"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"nkpV0yY3nQKz","executionInfo":{"status":"ok","timestamp":1743561397509,"user_tz":180,"elapsed":8,"user":{"displayName":"Lucas Lira","userId":"06092401788430501434"}}},"outputs":[],"source":["# Imports\n","import tensorflow as tf\n","#import torch\n","import transformers\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fc226qJ5nQK0","outputId":"a2f59417-05a7-49d0-83df-adee3e4cd5c1","executionInfo":{"status":"ok","timestamp":1743561401920,"user_tz":180,"elapsed":34,"user":{"displayName":"Lucas Lira","userId":"06092401788430501434"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Author: Lucas P. Lira\n","\n"]}],"source":["%reload_ext watermark\n","%watermark -a \"Lucas P. Lira\""]},{"cell_type":"markdown","metadata":{"id":"5lzuile_nQK0"},"source":["<!-- Projeto Desenvolvido na Data Science Academy - www.datascienceacademy.com.br -->\n","\n","## Funções Usadas Para Carregar Um LLM Pré-Treinado\n","\n","**GPT2LMHeadModel**: Esta é a classe modelo para o GPT-2 (Generative Pretrained Transformer 2), um modelo de linguagem poderoso que pode gerar texto semelhante ao humano. A parte 'LMHead' indica que esta variante do modelo inclui um cabeçote de modelagem de linguagem no topo do modelo GPT-2 básico, o que o torna adequado para tarefas como geração de texto.\n","\n","**GPT2Tokenizer**: Este é um tokenizer projetado especificamente para o modelo GPT-2. Um tokenizer é usado para converter texto em um formato que possa ser compreendido e processado pelo modelo. Para GPT-2, isso envolve a conversão de texto em tokens (como palavras ou subpalavras) e a codificação desses tokens como valores numéricos."]},{"cell_type":"markdown","metadata":{"id":"BVRSQ_1jnQK1"},"source":["## Carregando o Tokenizador\n","\n","https://huggingface.co/gpt2-large"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"IWXSIx89nQK1","executionInfo":{"status":"ok","timestamp":1743561804553,"user_tz":180,"elapsed":255,"user":{"displayName":"Lucas Lira","userId":"06092401788430501434"}}},"outputs":[],"source":["# Carrega o tokenizador\n","# device_map=\"auto\" permite que o modelo utilize automaticamente a GPU disponível, se houver uma.\n","# Caso contrário, ele usará a CPU.\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\", device_map=\"auto\")"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3uXwJ8XOnQK1","outputId":"e25a5bed-731b-413c-9265-2e5b90f9136c","executionInfo":{"status":"ok","timestamp":1743561809148,"user_tz":180,"elapsed":6,"user":{"displayName":"Lucas Lira","userId":"06092401788430501434"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPT2Tokenizer(name_or_path='gpt2-large', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","}\n",")"]},"metadata":{},"execution_count":30}],"source":["# Define o conjunto de regras de como os dados serão processados e retorna em um formato em que o modelo é capaz de processar.\n","tokenizer"]},{"cell_type":"markdown","metadata":{"id":"8CC_0uH1nQK2"},"source":["**from_pretrained**: Este é um método da classe GPT2Tokenizer. O método from_pretrained é usado para carregar um tokenizer que já foi treinado em um conjunto de dados específico. Neste caso, está treinado para trabalhar com o modelo GPT-2.\n","\n","**\"gpt2-large\"**: Este é um argumento de string para o método from_pretrained. Ele especifica qual variante do tokenizer do modelo GPT-2 você deseja usar. O modelo gpt2-large é uma versão maior do modelo GPT-2, o que significa que possui mais parâmetros e pode potencialmente gerar texto mais coerente e contextualmente relevante em comparação com versões menores. O tokenizer para gpt2-large é treinado especificamente para funcionar bem com esta variante de modelo."]},{"cell_type":"markdown","metadata":{"id":"Pu_EbSGJnQK2"},"source":["## Carregando o Modelo Pré-Treinado"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"GBcIMEzKnQK2","executionInfo":{"status":"ok","timestamp":1743561813447,"user_tz":180,"elapsed":1839,"user":{"displayName":"Lucas Lira","userId":"06092401788430501434"}}},"outputs":[],"source":["# Carrega o modelo\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\", device_map=\"auto\", pad_token_id = tokenizer.eos_token_id)"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bnbYTkyvnQK2","outputId":"1720b5c8-1977-4215-ec65-83ee38c25c06","executionInfo":{"status":"ok","timestamp":1743561815797,"user_tz":180,"elapsed":8,"user":{"displayName":"Lucas Lira","userId":"06092401788430501434"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 1280)\n","    (wpe): Embedding(1024, 1280)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-35): 36 x GPT2Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D(nf=3840, nx=1280)\n","          (c_proj): Conv1D(nf=1280, nx=1280)\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D(nf=5120, nx=1280)\n","          (c_proj): Conv1D(nf=1280, nx=5120)\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",")"]},"metadata":{},"execution_count":32}],"source":["model"]},{"cell_type":"markdown","metadata":{"id":"z4hlw_yinQK2"},"source":["**pad_token_id**: Este parâmetro define o token que o modelo usa para preenchimento. O preenchimento é usado para preencher sequências mais curtas para corresponder ao comprimento da sequência mais longa ao processar um lote de dados de texto.\n","\n","**tokenizer.eos_token_id**: O eos_token_id é o ID do token de 'fim da string' do tokenizer. Esta linha define o ID do token de preenchimento como igual ao ID do token de final de string. Ele informa ao modelo que o preenchimento deve ser tratado de forma semelhante ao token de final de string, o que é importante para manter a consistência na forma como o modelo processa sequências de texto."]},{"cell_type":"markdown","metadata":{"id":"PAu-brYcnQK3"},"source":["## Gerando Texto com o LLM"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"8cMpygCfnQK3","executionInfo":{"status":"ok","timestamp":1743561911925,"user_tz":180,"elapsed":2,"user":{"displayName":"Lucas Lira","userId":"06092401788430501434"}}},"outputs":[],"source":["# Texto inicial\n","prompt = 'What is Artificial Intelligence'"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"OqHL9GpfnQK3","executionInfo":{"status":"ok","timestamp":1743561912654,"user_tz":180,"elapsed":15,"user":{"displayName":"Lucas Lira","userId":"06092401788430501434"}}},"outputs":[],"source":["# Tokeniza o texto do prompt\n","input_ids = tokenizer.encode(prompt, return_tensors = 'pt')"]},{"cell_type":"markdown","metadata":{"id":"Rz3gqs1bnQK3"},"source":["**encode**: Este é um método do tokenizer. Ele recebe uma sequência de texto e a converte em uma lista de tokens numéricos. Cada token corresponde a uma palavra ou parte de uma palavra.\n","\n","**return_tensors='pt'**: Este argumento diz ao tokenizer para retornar os tokens em um formato adequado para PyTorch (indicado por 'pt')."]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XPBVi7yXnQK3","outputId":"38dc17fb-60fa-4d1c-b9e9-977fefff05e4","executionInfo":{"status":"ok","timestamp":1743561913950,"user_tz":180,"elapsed":9,"user":{"displayName":"Lucas Lira","userId":"06092401788430501434"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 2061,   318, 35941,  9345]])"]},"metadata":{},"execution_count":36}],"source":["# Visualiza\n","input_ids"]},{"cell_type":"code","source":["input_ids = input_ids.to('cuda')"],"metadata":{"id":"l7PeqjiFY-H_","executionInfo":{"status":"ok","timestamp":1743562015345,"user_tz":180,"elapsed":13,"user":{"displayName":"Lucas Lira","userId":"06092401788430501434"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A_pXIZJ_nQK3"},"source":["O tensor de saída representa um tensor PyTorch contendo uma sequência de tokens numéricos. Esses tokens são o resultado do processamento de texto por meio de um tokenizer, especificamente o tokenizer GPT-2 nesse caso.\n","\n","O tokenizer possui um vocabulário de tokens (palavras, partes de palavras, símbolos, etc.) e cada pedaço de texto exclusivo recebe um número específico. Por exemplo, a palavra “olá” pode ser transformada no número 1256, “mundo” pode se tornar 5678 e assim por diante.\n","\n","A sequência acima corresponde à forma tokenizada da frase de entrada. Cada número é mapeado para uma palavra ou parte de uma palavra nessa frase.\n","\n","Os colchetes duplos indicam que este é um tensor bidimensional. Neste contexto uma única sequência (uma frase) está sendo representada, portanto, apenas uma linha neste tensor 2D.\n","\n","Os valores numéricos não têm significado inerente sem o contexto do vocabulário do tokenizer. Para entender qual texto cada número representa, você precisaria procurar esses IDs no vocabulário do tokenizer.\n","\n","Esse formato de tensor é o que você normalmente alimenta em um modelo de aprendizado de máquina como o GPT-2 para executar tarefas como geração de texto, classificação ou resposta a perguntas. O modelo lê esses números e usa sua rede neural treinada para interpretá-los e gerar resultados apropriados com base em seu treinamento."]},{"cell_type":"code","execution_count":41,"metadata":{"id":"Zh6M7HbcnQK4","executionInfo":{"status":"ok","timestamp":1743562022890,"user_tz":180,"elapsed":5606,"user":{"displayName":"Lucas Lira","userId":"06092401788430501434"}}},"outputs":[],"source":["# Gerando o texto a partir do prompt com o texto inicial\n","texto_gerado = model.generate(input_ids,\n","                              max_length = 100,\n","                              num_beams = 5,\n","                              no_repeat_ngram_size = 2,\n","                              early_stopping = True)"]},{"cell_type":"markdown","metadata":{"id":"dElxztMjnQK4"},"source":["**max_length=100**: Este parâmetro define o comprimento máximo da sequência a ser gerada. O valor 100 inclui o comprimento do texto de entrada (o contexto) e o novo texto que o modelo irá gerar. O modelo para de gerar tokens adicionais quando esse comprimento é atingido.\n","\n","**num_beams=5**: Este parâmetro permite a pesquisa de feixe com 5 feixes. A pesquisa em feixe é uma técnica usada em PLN para geração de texto onde o modelo considera várias próximas palavras possíveis em cada etapa e mantém as sequências (ou \"feixes\") mais promissoras de tokens em cada etapa. Usar 5 feixes significa que o modelo rastreia 5 sequências potenciais em cada etapa, o que pode levar a resultados de maior qualidade, mas requer mais recursos computacionais.\n","\n","**no_repeat_ngram_size=2**: Esta configuração evita que o modelo repita os mesmos n-gramas (neste caso, sequências de 2 tokens) no texto de saída. Ajuda a reduzir a repetitividade no texto gerado.\n","\n","**early_stopping=True**: Este parâmetro informa ao modelo para parar de gerar texto assim que todos os candidatos ao feixe atingirem o token de final da frase. Pode tornar o processo de geração mais eficiente, interrompendo a busca assim que uma saída satisfatória for encontrada."]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rsmH5ePynQK4","outputId":"3f403a1d-e355-4a7e-e051-22d85412d1f9","executionInfo":{"status":"ok","timestamp":1743562027646,"user_tz":180,"elapsed":6,"user":{"displayName":"Lucas Lira","userId":"06092401788430501434"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 2061,   318, 35941,  9345,    30,   198,   198,  8001,  9542,  4430,\n","           357, 20185,     8,   318,   257,  8478,   286,  3644,  3783,   326,\n","          7529,   351,  3644,  4056,   326,   389,  1498,   284,  2193,   290,\n","          6068,   284,   511,  2858,    13,  9552,   468,   587,  1088,   329,\n","           257,   890,   640,    11,   475,   340,   318,   691,  2904,   326,\n","          9061,   423,  1716,  6007,   286,  9489,  8861,   326,   547,  1752,\n","          1807,   284,   307,  3675,   262,  9889,   286,  1692,  9791,    13,\n","          1114,  1672,    11,   257,  3644,  1430,   460,  2193,   703,   284,\n","           711,   257,  2008,   983,   416,  2712,   262,   976,   983,   625,\n","           290,   625,   757,    13,   770,   318,  1444, 37414,  4673,    13]],\n","       device='cuda:0')"]},"metadata":{},"execution_count":42}],"source":["texto_gerado"]},{"cell_type":"markdown","metadata":{"id":"LkhsPmA_nQK4"},"source":["Texto? Como assim?\n","\n","Isso é o que o modelo gera realmente. Vamos agora converter isso em algo que faça sentido."]},{"cell_type":"markdown","metadata":{"id":"xw3XRCEpnQK4"},"source":["## Decodificando o Resultado com o Tokenizador"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x5NQwHSOnQK4","outputId":"e55a8a43-867c-4196-f922-4129511193ea","executionInfo":{"status":"ok","timestamp":1743562034425,"user_tz":180,"elapsed":9,"user":{"displayName":"Lucas Lira","userId":"06092401788430501434"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["What is Artificial Intelligence?\n","\n","Artificial intelligence (AI) is a branch of computer science that deals with computer programs that are able to learn and adapt to their environment. AI has been around for a long time, but it is only recently that computers have become capable of performing tasks that were once thought to be beyond the capabilities of human beings. For example, a computer program can learn how to play a video game by playing the same game over and over again. This is called reinforcement learning.\n"]}],"source":["print(tokenizer.decode(texto_gerado[0], skip_special_tokens = True))"]},{"cell_type":"markdown","metadata":{"id":"X34gZOccnQK5"},"source":["O tokenizer não apenas converte texto em tokens (representações numéricas), mas também pode fazer o inverso – convertendo tokens de volta em texto.\n","\n","**skip_special_tokens=True**: Este argumento diz ao decodificador para ignorar tokens especiais, como tokens de preenchimento ou tokens de fim de sequência que são usados para processamento do modelo, mas não são significativos no texto final gerado."]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Nb0rtNlnQK5","outputId":"0e6d69bb-cfb6-4a55-d4e5-4f05be4cc194","executionInfo":{"status":"ok","timestamp":1743562042949,"user_tz":180,"elapsed":48,"user":{"displayName":"Lucas Lira","userId":"06092401788430501434"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Author: Lucas P. Lira\n","\n","Python implementation: CPython\n","Python version       : 3.11.11\n","IPython version      : 7.34.0\n","\n","tensorflow  : 2.19.0\n","torch       : 2.6.0+cu124\n","transformers: 4.50.3\n","\n"]}],"source":["%watermark -a \"Lucas P. Lira\" -v -p tensorflow,torch,transformers"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"wCPW4u2knQK5","outputId":"cb3ce7aa-699c-41e6-a02c-6aebbf944a20","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743562045730,"user_tz":180,"elapsed":57,"user":{"displayName":"Lucas Lira","userId":"06092401788430501434"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Python implementation: CPython\n","Python version       : 3.11.11\n","IPython version      : 7.34.0\n","\n","Compiler    : GCC 11.4.0\n","OS          : Linux\n","Release     : 6.1.85+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n"]}],"source":["%watermark -v -m"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"lnw0Wu3YnQK5","outputId":"278178b1-307b-499d-ecea-7c336af8de2d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743562047506,"user_tz":180,"elapsed":14,"user":{"displayName":"Lucas Lira","userId":"06092401788430501434"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["transformers: 4.50.3\n","tensorflow  : 2.19.0\n","\n"]}],"source":["%watermark --iversions"]},{"cell_type":"markdown","metadata":{"id":"jr83rcgFfUXc"},"source":["# Fim"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}